{
  "name": "com.unity.sentis",
  "displayName": "Sentis",
  "version": "1.6.0-pre.1",
  "unity": "2023.2",
  "description": "Sentis is a neural network inference library. It enables you to import trained neural network models, connect the network inputs and outputs to your game code, and then run them locally in your end-user app. Use cases include capabilities like natural language processing, object recognition, automated game opponents, sensor data classification, and many more.\n\nSentis automatically optimizes your network for real-time use to speed up inference. It also allows you to tune your implementation further with tools like frame slicing, quantization, and custom backend (i.e. compute type) dispatching.\n\nVisit https://unity.com/sentis for more resources.",
  "dependencies": {
    "com.unity.burst": "1.8.12",
    "com.unity.collections": "1.2.4",
    "com.unity.modules.imageconversion": "1.0.0"
  },
  "_upm": {
    "changelog": "### Added\n- SliceSet backend method for Concat and other operations.\n- Support for GridSample ONNX operator, Functional API and backend method, PaddingMode Enum for use with GridSample.\n- Methods to IModelStorage to allow retrieval of data types, tensor shapes and CPU values.\n- CPU backend to ExecutionContext for CPU fallback execution.\n- Optimization that replaces a Gather with a single index by a Split or Narrow.\n- Functional RandomChoice methods similar to numpy.\n- BitonicSort method for fast GPU sorting.\n\n### Changed\n- Model Input, Output, Layer and Constant indices are stored as integers rather than strings.\n- Min and Max no longer take more than 2 inputs in the backend, for more inputs use repeated application of the backend calls.\n- Optimized loading of compute functions for performance.\n- Reduced CPU allocations inside operations to avoid GC.\n- CPU fallback pass runs at worker instantiation rather than being serialized to the model.\n\n### Fixed\n- Many issues with Tensors of zero length and uploading/downloading data. They no longer have null backendData.\n- Issue where multiple Random layers were sometimes incorrectly collapsed to single layer by optimization pass.\n- NonMaxSuppression to have fast inference on CPU and GPUCompute backends.\n- Error messages for model deserialization.\n- Slice inference issues for slices of length 0.\n\n### Removed\n- Mean, Sum, Concat from the backend. Add, ScalarMad and SliceSet operations are used instead.\n- Unnecessary 'keepdim' argument from Reduce backend ops.\n- Constructor for Constant with a tensor argument.\n- CompleteOperationsAndDownload and similar methods from Tensor."
  },
  "upmCi": {
    "footprint": "44c9da1e07094697d072e451eaf636a323a9e053"
  },
  "documentationUrl": "https://docs.unity3d.com/Packages/com.unity.sentis@1.6/manual/index.html",
  "repository": {
    "url": "https://github.cds.internal.unity3d.com/unity/UnityInferenceEngine.git",
    "type": "git",
    "revision": "da61bfca399647f0c789ee715f0079890dfade83"
  },
  "samples": [
    {
      "displayName": "Convert tensors to textures",
      "description": "Examples of converting tensors to textures.",
      "path": "Samples~/Convert tensors to textures"
    },
    {
      "displayName": "Convert textures to tensors",
      "description": "Examples of converting textures to textures.",
      "path": "Samples~/Convert textures to tensors"
    },
    {
      "displayName": "Copy a texture tensor to the screen",
      "description": "An example of using TextureConverter.RenderToScreen to copy a texture tensor to the screen.",
      "path": "Samples~/Copy a texture tensor to the screen"
    },
    {
      "displayName": "Do an operation on a tensor",
      "description": "An example of using `IBackend` to do an operation on a tensor.",
      "path": "Samples~/Do an operation on a tensor"
    },
    {
      "displayName": "Encrypt a model",
      "description": "Example of serializing an encrypted model to disk using a custom editor window and loading that encrypted model at runtime.",
      "path": "Samples~/Encrypt a model"
    },
    {
      "displayName": "Quantize a model",
      "description": "Example of serializing a quantized model to disk using a custom editor window and loading that quantized model at runtime.",
      "path": "Samples~/Quantize a model"
    },
    {
      "displayName": "Read output asynchronously",
      "description": "Examples of reading the output from a model asynchronously, using compute shaders or Burst.",
      "path": "Samples~/Read output asynchronously"
    },
    {
      "displayName": "Run a model",
      "description": "Examples of running models with different numbers of inputs and outputs.",
      "path": "Samples~/Run a model"
    },
    {
      "displayName": "Run a model a layer at a time",
      "description": "An example of using StartManualSchedule to run a model a layer a time.",
      "path": "Samples~/Run a model a layer at a time"
    },
    {
      "displayName": "Use Burst to write data",
      "description": "An example of using Burst to write data to a tensor in the Job system.",
      "path": "Samples~/Use Burst to write data"
    },
    {
      "displayName": "Use a compute buffer",
      "description": "An example of using a compute shader to write data to a tensor on the GPU.",
      "path": "Samples~/Use a compute buffer"
    },
    {
      "displayName": "Use tensor indexing methods",
      "description": "Examples of using tensor indexing methods to get and set tensor values.",
      "path": "Samples~/Use tensor indexing methods"
    },
    {
      "displayName": "Use the functional API with an existing model",
      "description": "An example of using the functional API to extend an existing model.",
      "path": "Samples~/Use the functional API with an existing model"
    }
  ]
}
